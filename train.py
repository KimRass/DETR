"""
"s. We train DETR with AdamW [26] setting the initial trans-
former’s learning rate to 10−4 , the backbone’s to 10−5 , and weight decay to 10−4 .
"We train DETR using AdamW [26] with improved weight decay handling, set to
10−4 . We also apply gradient clipping, with a maximal gradient norm of 0.1."
"""